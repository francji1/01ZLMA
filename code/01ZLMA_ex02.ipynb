{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francji1/01ZLMA/blob/main/code/01ZLMA_ex02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-i6MbNFm4Zt"
      },
      "source": [
        "# 01ZLMA - Exercise 02\n",
        "Exercise 02 of the course 01ZLMA.\n",
        "\n",
        "## Contents\n",
        "\n",
        "* Likelihood, Score, Fisher Information\n",
        " ---\n",
        "* Numeric methods of MLE calculations\n",
        " ---\n",
        "* Newton-Raphson\n",
        "* Fiher scoring\n",
        "* IWLS\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "593Mg4ZbbeEE"
      },
      "source": [
        "#  Theory recap from Lectures 01-03\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularity Conditions"
      ],
      "metadata": {
        "id": "TA6PfEOKfdi5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- Let $(\\mathcal{Y},\\,\\mathcal{B},\\,P_\\theta)_{\\theta\\in\\Theta}$ be a parametric family of probability spaces, where  \n",
        "  $\\mathcal{Y} \\subset \\mathbb{R}$ is a sample space,  \n",
        "  $\\mathcal{B}$ is a Borel $\\sigma$-algebra of subsets of $\\mathcal{Y}$,  \n",
        "  $\\Theta \\subset \\mathbb{R}^m$ is a parametric space.\n",
        "\n",
        "- Assume $f(y;\\,\\theta)$ is a probability density function with respect to a $\\sigma$-finite measure $\\mu$  \n",
        "  (*Lebesgue measure in the continuous case and counting measure in the discrete case*), satisfying:\n",
        "\n",
        "**(M1)** The set $A = \\{y \\in \\mathcal{Y} : f(y;\\theta) > 0\\}$ does not depend on $\\theta$.\n",
        "\n",
        "**(M2)** If $\\theta_1 \\neq \\theta_2$, then\n",
        "$$\n",
        "\\mu(\\{y \\in A : f(y;\\theta_1) = f(y;\\theta_2)\\}) = 0.\n",
        "$$\n",
        "\n",
        "We consider densities that satisfy the following **regularity conditions**:\n",
        "\n",
        "**(R1)** For every parameter $\\theta_0 \\in \\Theta$, there exists a neighborhood $N(\\theta_0) \\subset \\Theta$ in which, for almost every $y \\in A$ (with respect to $\\mu$), the following derivatives exist:\n",
        "$$\n",
        "\\frac{\\partial f(y;\\theta)}{\\partial \\theta_p}, \\quad\n",
        "\\frac{\\partial^2 f(y;\\theta)}{\\partial \\theta_p \\partial \\theta_q}, \\quad\n",
        "\\frac{\\partial^3 f(y;\\theta)}{\\partial \\theta_p \\partial \\theta_q \\partial \\theta_r}, \\quad p,q,r = 1,\\dots,m,\n",
        "$$\n",
        "and they are continuous and finite.\n",
        "\n",
        "**(R2)** There exist constants $0 < L < \\infty$ and real-valued functions $F(y)$, $G(y)$, and $H(y)$ (possibly dependent on $\\theta_0$), such that for $p,q,r = 1,\\dots,m$, for almost every $y \\in A$ and for all $\\theta \\in N(\\theta_0)$ it holds:\n",
        "$$\n",
        "\\left|\\frac{\\partial f(y;\\theta)}{\\partial \\theta_p}\\right| < F(y), \\quad\n",
        "\\left|\\frac{\\partial^2 f(y;\\theta)}{\\partial \\theta_p \\partial \\theta_q}\\right| < G(y), \\quad\n",
        "\\left|\\frac{\\partial^3 \\ln f(y;\\theta)}{\\partial \\theta_p \\partial \\theta_q \\partial \\theta_r}\\right| < H(y),\n",
        "$$\n",
        "where $F$ and $G$ are $\\mu$-integrable and\n",
        "$$\n",
        "\\int_A H(y)f(y;\\theta)\\,d\\mu(y) < L.\n",
        "$$\n",
        "\n",
        "**(R3)** Fisher information matrix $I(\\theta)$ is finite and positive definite for all $\\theta \\in \\Theta$.\n"
      ],
      "metadata": {
        "id": "Ak81LAvXglxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exponential family of probability distributions"
      ],
      "metadata": {
        "id": "cfGaSbRDgFSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "  1. realization $(y_i,\\ldots,y_n)$ of $iid$ random variables $Y_1,\\ldots,Y_n$ with probability density function $f(y;\\theta;\\phi)$ from an exponential family of probability distributions\n",
        "  $$f(y;\\theta;\\phi) = exp\\left(\\frac{y \\theta - b(\\theta)}{a(\\phi)} - c(y,\\phi)\\right),$$\n",
        "  where conditions of regularity are fulfilled (one dimensional case, i.e. $y_i,\\theta_i \\in R, a(\\phi) >0, \\phi >0)$.\n",
        "  2. Regression matrix $X$ and vector of unknown parameters $\\beta$, linear predictor $Î· = X \\beta$\n",
        "  3. A link function $g(x)$\n",
        "  $$\\eta_i = g(\\mu_i) = x_i^T \\beta, \\ \\text{where} \\ \\mu_i = E[Y_i] \\ \\ i = 1,\\ldots,n$$\n",
        "\n",
        "The dispersion $a(\\phi)$ is typically known. If not, we take it as nuisance parameter.\n",
        "\n",
        "Link function satisfying $g(\\mu_i) = \\theta_i$ is called canonical."
      ],
      "metadata": {
        "id": "h1bx0ptRgoA5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpQUiXf-QkGA"
      },
      "source": [
        "For $b(\\theta) \\in C^2$ we showed:\n",
        "$$E[Y] = b'(\\theta) $$\n",
        "$$V[Y] = a(\\phi) b''(\\theta) $$\n",
        "and defined variance function $v(\\mu) = \\frac{\\partial \\mu}{\\partial \\theta}$, i.e. $V[Y] = a(\\phi) v(\\mu)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcJB1uOTZEBZ"
      },
      "source": [
        "Relations:\n",
        "\n",
        "$$\n",
        "\\beta \\xrightarrow[]{\\eta_i = x_i^T\\beta} \\eta\n",
        "\\xrightarrow[]{\\mu_i = g^{-1}(\\eta_i)}  \\mu\n",
        "\\xrightarrow[]{\\theta_i = (b')^{-1}(\\mu_i)}  \\theta\n",
        "$$\n",
        "\n",
        "Inverse relatiions\n",
        "$$\n",
        "\\eta_i\n",
        "\\xleftarrow[]{}  \\mu\n",
        "\\xleftarrow[]{}  \\theta\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd7Z1sV3YNOm"
      },
      "source": [
        "## Likelihood, Score function, Information matrix\n",
        "(Under conditions M1,M2 and regularity conditions R1-R3 from the lecture)\n",
        "\n",
        "* Likelihood function:\n",
        "$$L_n(\\Theta) = L_n(\\Theta|Y) = \\prod_{i=1}^{n} f(y_i;\\Theta) $$\n",
        "* log-likelihood function:\n",
        "$$l_n(\\Theta) = l_n(\\Theta|Y) = \\sum_{i=1}^{n} \\text{ln} f(y_i;\\Theta) $$\n",
        "* Score function (theoretical construct, gradient of the log-likelihood with respect to the parameter vector $\\Theta$):\n",
        "$$U = U(y; \\Theta) = \\frac{\\partial \\text{ln} f(y ; \\Theta)}{\\partial \\Theta} $$\n",
        "* Score vector (statistic used in inference):\n",
        "$$U_{n} =  \\sum_{i=1}^{n} U(y_i;\\Theta)= \\sum_{i=1}^{n} \\frac{\\partial l_i(y_i ; \\Theta)}{\\partial \\Theta} $$\n",
        "* Fisher Information matrix\n",
        "$$ I_n (\\Theta) = E_{\\theta}[U_nU_n^T]$$\n",
        "with elements\n",
        "$$ I_{n,j,k} = E_{\\theta}[\\frac{\\partial l}{\\partial \\theta_j}\\frac{\\partial l}{\\partial \\theta_k}] = -E_{\\theta}[\\frac{\\partial^2 l}{\\theta_j \\theta_k}]$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bernoulli example and task:\n",
        "\n",
        "* Interpret Score on Bernoulli process with N successes and M failures, where the probability of success is $\\theta$. What does it mean if Score is greater than zero?\n",
        "* Why is the second derivative called information (use again $E_{\\theta}[U] = 0$)?\n",
        "\n",
        "Note: The choice of the likelihood function is similar to choice of a prior in Bayesian analysis. (https://stats.stackexchange.com/questions/196576/what-kind-of-information-is-fisher-information?noredirect=1&lq=1)\n"
      ],
      "metadata": {
        "id": "mOWsm4rFs3FK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add previous question:\n",
        "\n",
        "Likelihood function of r.v. $X$ with Bernoulli distribution and parameter $p \\in (0,1)$:\n",
        "$$L_n(\\Theta) = L_n(\\Theta|X) = \\prod_{i=1}^{n} f(x_i;\\Theta) = \\prod_{i=1}^{n} p^{x_i}(1-p)^{1-x_i} \\ $$\n",
        "Log-likelihood function of Bernoulli distribution:\n",
        "$$l_n(\\Theta) = \\sum_{i=1}^{n} \\text{ln} f(x_i;\\Theta) = ... $$\n",
        "\n",
        "Score function of Bernoulli distribution:\n",
        "$$U = U(x_i;\\Theta) = \\frac{\\partial \\text{ln} f(x_i ;\\Theta)}{\\partial \\Theta}  = ... $$\n",
        "MLE estimation of the parameter $p$:\n",
        "$$\\hat{p}_{MLE} = \\frac{y}{n} $$\n",
        "\n"
      ],
      "metadata": {
        "id": "ZICKN8ySi76R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "FVCvsHooA4Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the R magic extension\n",
        "%load_ext rpy2.ipython"
      ],
      "metadata": {
        "id": "iDH6ZJdadoOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your turn in python:"
      ],
      "metadata": {
        "id": "GY-9pq7e2hLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulation of Bernoulli distribution and computation of Score function:\n",
        "n = 9\n",
        "p = 0.4\n",
        "x = np.random.binomial(1, p, n)\n",
        "y = np.sum(x)\n",
        "p_hat = y / n\n",
        "print(\"np= \", n * p)\n",
        "print(\"p= \", p)\n",
        "print(\"y= \", y)\n",
        "print(\"p_hat = \",p_hat)\n",
        "# U = ... # fill\n",
        "print(\"U= \",U)\n",
        "\n",
        "# Define the score function U and the log-likelihood function l\n",
        "def U(y, p=p, n=n):\n",
        "    return # fill\n",
        "\n",
        "def l(p, y=y, n=n):\n",
        "    return # fill\n",
        "\n",
        "# Plotting\n",
        "# Score function U\n",
        "y_values = np.arange(0, n+1)  # Y values\n",
        "U_values = [U(y_val) for y_val in y_values]  # U values for each Y\n",
        "\n",
        "plt.figure(figsize=[12, 6])\n",
        "plt.subplot(1, 2, 1)  # First subplot\n",
        "plt.plot(y_values, U_values, label='Score function U')\n",
        "plt.axvline(x=n*p, color='red', linestyle='--', label='np')\n",
        "plt.xlabel('Y')\n",
        "plt.ylabel('U')\n",
        "plt.title('Score Function U(Y)')\n",
        "plt.legend()\n",
        "\n",
        "# Log-likelihood function l\n",
        "p_values = np.linspace(0.01, 0.99, 1000)\n",
        "l_values = [l(p_val) for p_val in p_values]  # l values for each p\n",
        "\n",
        "plt.subplot(1, 2, 2)  # Second subplot\n",
        "plt.plot(p_values, l_values, label='Log-likelihood function l')\n",
        "plt.axvline(x=p_hat, color='green', linestyle='--', label='p_hat')\n",
        "plt.xlabel('p')\n",
        "plt.ylabel('l')\n",
        "plt.title('Log-likelihood Function l(p)')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YHcV-wCcACtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rhklzdz6_GLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wV3e4NaLbVZg"
      },
      "source": [
        "%%R\n",
        "library(tidyverse)\n",
        "library(lubridate)\n",
        "#library(MASS)\n",
        "#library(rmarkdown)\n",
        "\n",
        "#install.packages(\"plotly\")\n",
        "#library(plotly)\n",
        "\n",
        "#install.packages(\"car\")\n",
        "#library(car)\n",
        "#install.packages(\"GGally\")\n",
        "#library(GGally)\n",
        "\n",
        "#For sure: set dplyr functions\n",
        "select    <- dplyr::select;\n",
        "rename    <- dplyr::rename;\n",
        "mutate    <- dplyr::mutate;\n",
        "summarize <- dplyr::summarize;\n",
        "arrange   <- dplyr::arrange;\n",
        "slice     <- dplyr::slice;\n",
        "filter    <- dplyr::filter;\n",
        "recode    <- dplyr::recode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxEycwoDWr1H"
      },
      "source": [
        "Datasets we will use:\n",
        "\n",
        "Peter K. Dunn â¢ Gordon K. Smyth, Generalized Linear ModelsWith Examples in R\n",
        "\n",
        "https://link.springer.com/content/pdf/10.1007%2F978-1-4419-0118-7.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtjmAUwlXCxB"
      },
      "source": [
        "%%R\n",
        "install.packages(\"GLMsData\")\n",
        "library(GLMsData)\n",
        "#install.packages(\"splines\")\n",
        "#library(splines)\n",
        "#install.packages(\"statmod\")\n",
        "#library(statmod)\n",
        "#install.packages(\"tweedie\")\n",
        "#library(tweedie)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azYBX7-3XXVw"
      },
      "source": [
        "A. J. Dobson AN INTRODUCTION TO GENERALIZED LINEAR MODELS\n",
        "\n",
        "https://link.springer.com/content/pdf/10.1007%2F978-1-4419-0118-7.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_t4k-_RXXoZ"
      },
      "source": [
        "%%R\n",
        "options(warn=-1)\n",
        "install.packages(\"dobson\")\n",
        "library(dobson)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCR1hTq70SeK"
      },
      "source": [
        "CRAN packages for generalized linear models and with related methods\n",
        "\n",
        "https://cran.r-project.org/web/packages/cranly/vignettes/glms.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf9DWDSoXYtd"
      },
      "source": [
        "%%R\n",
        "#data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeGgMpKWX1EE"
      },
      "source": [
        "## Estimate parameters $\\beta$ by MLE\n",
        "\n",
        "log-likelihood function is\n",
        "$$ l(\\theta, \\phi, y) = \\sum_{i=1}^n \\frac{y_i \\theta_i - b(\\theta_i)}{a_i(\\phi)} + \\sum_{i=1}^n c(y_i,\\phi) $$\n",
        "and we want to estimate $\\beta = (\\beta_1, \\ldots, \\beta_n)^T$, i.e.\n",
        "$$ \\hat{\\beta} = argmax_{\\beta}(l(\\theta,\\phi,y))$$\n",
        "$$\\Rightarrow$$\n",
        "$$ U_n = \\sum_{i=1}^n \\frac{y_i - \\mu_i}{V[Y_i]  g'(\\mu_i)} x_i = X^T M^{-1}(y-\\mu) = 0$$\n",
        "where $M = diag(V[Y_i]g'(\\mu_i))$\n",
        "$$\\Rightarrow$$\n",
        "$$ U_n(\\beta) = X^T W(\\beta)^{-1}Z(\\beta),$$\\\n",
        "where $W(\\beta) = diag(V[Y_i]g'(\\mu_i)^2)$ and $Z(\\beta) = diag(g'(\\mu_i)(y-\\mu))$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB8eoMx10gQN"
      },
      "source": [
        "It can be shown\n",
        "$$I_n = X^T W(\\beta)^{-1} X $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKJ3w-ZY1WAS"
      },
      "source": [
        "### Newton Rapson\n",
        "\n",
        "For MLE using the score function, the estimating equation is\n",
        "$$\\hat{\\beta}^{(r+1)} = \\hat{\\beta}^{(r)} + \\left(  \\frac{d U(\\hat{\\beta}^{(r)})}{d \\beta} \\right)^{-1} U(\\hat{\\beta}^{(r)})$$\n",
        "\n",
        "**Question**: What are advantages and disadvantages of this method. How do we call matrix $\\frac{d U(\\hat{\\beta}^{(r)})}{d \\beta}$?\n",
        "\n",
        "* The matrix $\\frac{d U(\\hat{\\beta}^{(r)})}{d \\beta}$ is called the observed Hessian (matrix of second derivatives of log-likelihood).\n",
        "\n",
        "Advantages of Newton-Raphson:\n",
        "* Fast convergence (quadratic convergence) when close to the optimum.\n",
        "* Precise because it uses the actual curvature (observed Hessian).\n",
        "\n",
        "Disadvantages:\n",
        "* Computationally expensive for large parameter spaces (computing the exact Hessian is demanding).\n",
        "* Can be unstable if the Hessian is nearly singular or if the initial guess is far from the solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLJSX9Vr8gwe"
      },
      "source": [
        "### Fisher Scoring\n",
        "\n",
        "$$\\hat{\\beta}^{(r+1)} =  \\hat{\\beta}^{(r)} + \\left( E \\left[- \\frac{d U(\\hat{\\beta}^{(r)})}{d \\beta} \\right] \\right)^{-1} U(\\hat{\\beta}^{(r)})  = \\hat{\\beta}^{(r)} + I(\\hat{\\beta}^{(r)})^{-1} U(\\hat{\\beta}^{(r)})$$\n",
        "\n",
        "**Question**: What are advantages and disadvantages of this method.\n",
        "\n",
        "Advantages of Fisher Scoring:\n",
        "* More stable than Newton-Raphson, especially if the observed Hessian fluctuates significantly.\n",
        "* Does not require recalculating Hessian for every iteration, thus computationally simpler and more efficient.\n",
        "\n",
        "Disadvantages:\n",
        "* Typically slower convergence (linear convergence) compared to Newton-Raphsonâs quadratic convergence.\n",
        "* Efficiency depends on how well the Fisher information (expected Hessian) approximates the actual Hessian."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9WHH77u97dn"
      },
      "source": [
        "### IWLS\n",
        "Transforms the GLM estimation into a series of weighted least squares problems:\n",
        "\n",
        "$$I(\\hat{\\beta}^{(r)}) \\hat{\\beta}^{(r+1)}  =  I(\\hat{\\beta}^{(r)}) \\hat{\\beta}^{(r)} + U(\\hat{\\beta}^{(r)})$$\n",
        "$$ \\Rightarrow$$\n",
        "$$(X^T W(\\hat{\\beta}^{(r)})^{-1} X) \\hat{\\beta}^{(r+1)}  = X^T W(\\hat{\\beta}^{(r)})^{-1} Z(\\hat{\\beta}^{(r)})  $$\n",
        "\n",
        "where\n",
        "* $W(Î²)$ is a diagonal matrix of weights derived from variance and link function.\n",
        "* $ð(ð½)$ is the adjusted response (working response).\n",
        "\n",
        "\n",
        "Advantages of IWLS:\n",
        "* Computationally efficient (only involves solving weighted linear regression at each step).\n",
        "* Numerically stable and easy to implement, widely used in practice.\n",
        "\n",
        "Disadvantages:\n",
        "* Convergence slower than Newton-Raphson (usually linear rate).\n",
        "* Sensitivity to poor initial guesses and potential convergence issues in highly nonlinear scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vuRvxDiYAoe"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d4KK-T_YA_Q"
      },
      "source": [
        "# Poisson example (Dobson 4.4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate the dataset\n",
        "X = np.array([-1, -1, 0, 0, 0, 0, 1, 1, 1])\n",
        "Y = np.array([2, 3, 6, 7, 8, 9, 10, 12, 15])\n",
        "n = len(X)\n",
        "\n",
        "# Plot the dataset\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(X, Y, color='red', s=60)\n",
        "plt.xlim(-1.5, 1.5)\n",
        "plt.ylim(0, 16)\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.title(\"Scatter Plot of Dataset\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EcSDRHSLZKCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-7NJWjGLZIdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGs3-yFPADyY"
      },
      "source": [
        "Let us assume that the response $Y_i$ are Poisson random variables and model the relation ship between $Y_i$ and $x_i$ by the straight line, i.e.\n",
        "$$\n",
        "E[Y_i] = \\mu_i = \\beta_1 + \\beta_2 x_i = x_i^T \\beta \\  \\Rightarrow \\ q(\\mu_i) = \\mu_i =  x_i^T \\beta  = \\eta_i\n",
        "$$\n",
        "Therefore\n",
        "$$\n",
        "\\frac{1}{g'(\\mu_i)} = 1 \\ \\Rightarrow \\ w_{ii} = ...\n",
        "$$\n",
        "$$\n",
        "z_i =  ...\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UVZoB1t6ZTyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python code"
      ],
      "metadata": {
        "id": "ybt0ZrpeZo-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "nSBjjZnyaOkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate weights\n",
        "def calc_W(X, beta):\n",
        "    n = X.shape[0]\n",
        "    w = np.diag(1 / (X @ beta))\n",
        "    return w\n",
        "\n",
        "# Initial values\n",
        "X = np.column_stack((np.ones(n), X))  # Adding a column of ones for intercept\n",
        "beta_0 = np.array([7, 5])\n",
        "print(\"beta_0 = \", beta_0)\n",
        "z = Y\n",
        "\n",
        "# Compute Weight matrix\n",
        "W = calc_W(X, beta_0)\n",
        "var_covar_matrix = np.linalg.inv(X.T @ W @ X)\n",
        "print(\"var_covar_matrix_0\")\n",
        "print(var_covar_matrix)\n",
        "\n",
        "# Step from beta_0 to beta_1\n",
        "beta_1 = np.linalg.solve(X.T @ W @ X, X.T @ W @ z)\n",
        "print(\"beta_1 = \", beta_1)\n",
        "\n",
        "# Variance-covariance matrix for estimates beta_1\n",
        "W = calc_W(X, beta_1)\n",
        "var_covar_matrix = np.linalg.inv(X.T @ W @ X)\n",
        "print(\"var_covar_matrix_1\")\n",
        "print(var_covar_matrix)\n",
        "\n",
        "# Step from beta_0 to beta_1\n",
        "beta_2 = np.linalg.solve(X.T @ W @ X, X.T @ W @ z)\n",
        "print(\"beta_2 = \", beta_1)"
      ],
      "metadata": {
        "id": "gjGVoIlKA9kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAmB4PDZJKEl"
      },
      "source": [
        "### Your turn:\n",
        "1. Write function to calculate IWLS for example 1 with following parameters\n",
        " * maximal number of iteration\n",
        " * accuracy\n",
        " * initial estimation\n",
        "2. Try different initialization (ols, random, lecture, ...). Plot convergence of parameters $\\beta_i$ depends on initial estimation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Change the function to be able to change maxiter and epsilon in the input.\n",
        "* plot betas and its evolution"
      ],
      "metadata": {
        "id": "qPZYzFpE4x-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to calculate weights, assuming a linear model for simplicity\n",
        "def calc_W(X, beta):\n",
        "    return np.eye(X.shape[0])\n",
        "\n",
        "# IWLS function\n",
        "def IWLS(X, Y, beta_init, max_iter, accuracy):\n",
        "    beta = beta_init\n",
        "    beta_history = [beta]\n",
        "    for i in range(max_iter):\n",
        "        W = calc_W(X, beta)  # Update weights, modify this function based on actual model\n",
        "        beta_new = # fill\n",
        "        beta_history.append(beta_new)\n",
        "        # Check for convergence\n",
        "        if np.linalg.norm(beta_new - beta) < accuracy:\n",
        "            break\n",
        "        beta = beta_new\n",
        "    return np.array(beta_history)\n",
        "\n"
      ],
      "metadata": {
        "id": "tb3WaRGl4xeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializations\n",
        "beta_ols = np.linalg.inv(X.T @ X) @ X.T @ Y  # OLS estimation\n",
        "beta_random = np.random.rand(2)  # Random initialization\n",
        "beta_lecture = np.array([7, 5])  # Provided initial values\n",
        "\n",
        "# Parameters\n",
        "max_iter = 10\n",
        "accuracy = 1e-9\n",
        "\n",
        "# Run IWLS with different initializations\n",
        "history_ols = IWLS(X, Y, beta_ols, max_iter, accuracy)\n",
        "history_random = IWLS(X, Y, beta_random, max_iter, accuracy)\n",
        "history_lecture = IWLS(X, Y, beta_lecture, max_iter, accuracy)\n",
        "\n",
        "# Plotting convergence\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(history_ols[:, 0], label='Î²0 from OLS', marker='o')\n",
        "plt.plot(history_ols[:, 1], label='Î²1 from OLS', marker='x')\n",
        "plt.plot(history_random[:, 0], label='Î²0 from Random', marker='o')\n",
        "plt.plot(history_random[:, 1], label='Î²1 from Random', marker='x')\n",
        "plt.plot(history_lecture[:, 0], label='Î²0 set as 7', marker='o')\n",
        "plt.plot(history_lecture[:, 1], label='Î²1 set as 5', marker='x')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Parameter value')\n",
        "plt.title('Convergence of Parameters in IWLS')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Y_1PGCRTaYwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXv_fgQP1teL"
      },
      "source": [
        "# By default R function\n",
        "%%R\n",
        "model <- glm(Y~-1+X, family=poisson(link = \"identity\"))\n",
        "summary(model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZh0p0TSUFtu"
      },
      "source": [
        "# By statsmodels in Python\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Fit the Poisson regression model\n",
        "model = sm.GLM(Y, X, family=sm.families.Poisson(link=sm.genmod.families.links.Identity()))\n",
        "results = model.fit()\n",
        "\n",
        "# Print the summary of the model\n",
        "print(results.summary())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1fGjyJW1sd8"
      },
      "source": [
        "# Poisson example (Dobson ex. 4.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IQ58Goz6hkm"
      },
      "source": [
        "\n",
        "An example 4.1. from the book \"An Introduction to Generalized Linear Models\" by Dobson. The data in Table 4.5 show the numbers of cases of AIDS in Australia by date of diagnosis for successive 3-months periods from 1984 to 1988.\n",
        "\n",
        "* Plot the number of cases $y_i$ against time period (i = 1 ... 20).\n",
        "* Use the Poisson distribution with parameter $\\lambda_i = i^{\\theta}$ or equivalently $log(\\lambda_i) = \\theta log(i)$. Plot $log(y_i)$ against $log(i)$ to examne this model.\n",
        "* Fit a generalized linear model to these data using the Poisson distribution and the log-link function, ie\n",
        "$$ g(\\lambda_i) = log(\\lambda_i) = \\beta_1 + \\beta_2 x_i.$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8FrflnA8TGz"
      },
      "source": [
        "%%R -o aids\n",
        "data(aids)\n",
        "AIDS <- aids %>%\n",
        "  mutate(season = paste0(year,\":Q\",quarter),\n",
        "         time = yq(season))\n",
        "\n",
        "X <- seq(1,20,1)\n",
        "Y <- aids$cases\n",
        "#Y <- c(1, 6, 16, 23, 27, 39, 31, 30, 43, 51, 63, 70, 88, 97, 91, 104, 110, 113, 149, 159)\n",
        "summary(aids)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDt8D8bc8VAd"
      },
      "source": [
        "%%R\n",
        "p <- ggplot(AIDS, aes(x=time, y=cases)) +\n",
        "  geom_line() #+\n",
        "  #scale_x_date(date_labels = \"%Y-%B\")+\n",
        "  #theme(axis.text.x=element_text(angle=60, hjust=1)) +\n",
        "  #scale_x_date(breaks = AIDS$time)\n",
        "p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1k90EyJ9YOE"
      },
      "source": [
        "aids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Create data\n",
        "X = np.arange(1, 21, 1)\n",
        "#Y = np.array([1, 6, 16, 23, 27, 39, 31, 30, 43, 51, 63, 70, 88, 97, 91, 104, 110, 113, 149, 159])\n",
        "Y = aids['cases']\n",
        "\n",
        "# Create a DataFrame similar to df2 in R\n",
        "df2 = pd.DataFrame({\n",
        "    'x': np.concatenate([X, np.log(X), np.log(X)]),\n",
        "    'y': np.concatenate([Y, Y, np.log(Y)]),\n",
        "    'group': ['X vs. Y'] * 20 + ['log(X) vs. Y'] * 20 + ['log(X) vs. log(Y)'] * 20\n",
        "})\n",
        "\n",
        "# Create the scatter plot\n",
        "fig = px.scatter(df2, x='x', y='y', color='group', title=\"Scatter Plot by plotly\")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "e-TeX7mJtj8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ty74jhWTzRL"
      },
      "source": [
        "### HW:\n",
        "* Write function to estimate coefficients by Newton-Rapson, Fisher-scoring and IWLS?\n",
        "* How these three approaches differ in this given example?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcDobOsvqyxX"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}