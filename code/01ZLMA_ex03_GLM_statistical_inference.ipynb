{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francji1/01ZLMA/blob/main/code/01ZLMA_ex03_GLM_statistical_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-i6MbNFm4Zt"
      },
      "source": [
        "# 01ZLMA - Exercise 03\n",
        "Exercise 03 of the course 01ZLMA.\n",
        "\n",
        "## Contents\n",
        "\n",
        "* Statistical Inference\n",
        " ---\n",
        "* Testing\n",
        " ---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "593Mg4ZbbeEE"
      },
      "source": [
        "#  Necessary theory recap from Lecture 04\n",
        "\n",
        "Under the conditions of regularity holds\n",
        "\n",
        "1.  $ \\ U(\\beta) \\sim N_{p}(0,I(\\beta)) \\Rightarrow  I^{-\\frac{1}{2}}(\\beta)\\, U(\\beta) {\\stackrel{D}{\\longrightarrow}} N_{p}(0, 1)$\n",
        "2. $ U(\\beta)I^{-1}(\\beta)U(\\beta)\\sim \\chi^{2}(p) \\Rightarrow U(\\beta)^T I^{-1}(\\beta)U(\\beta)  {\\stackrel{D}{\\longrightarrow}} \\chi^{2}(p)$\n",
        "3. Consistency of $\\hat{\\beta}$ and Wald statistics: \\\\\n",
        " $\\hat{\\beta}\\sim N_{p}(\\beta,I^{-1}(\\beta)) \\Rightarrow\n",
        "(\\hat{\\beta}-\\beta)^T I(\\beta)(\\hat{\\beta}-\\beta) {\\stackrel{D}{\\longrightarrow}} \\chi^{2}(p)$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2c7jDIXSGGL"
      },
      "source": [
        "Saturated and null model\n",
        "\n",
        "* Null model: $\\mu_i = \\mu, \\forall i \\in \\{1, \\ldots , n\\}$ \\\\\n",
        "The Null Model assumes one parameter for all of the data points, which means you only estimate 1 parameter.\n",
        "* Saturated model: $Y_i = \\hat{\\mu_i}, \\forall i \\in \\{1, \\ldots , n\\}$ \\\\\n",
        "The Saturated Model is a model that assumes each data point has its own parameters, which means you have n parameters to estimate.\n",
        "* Proposed Model:  model, where you try to explain your data points with $p$ parameters + an intercept term, so you have p+1 parameters, where $1 \\leq p \\leq n$.\n",
        "\n",
        "Questions:\n",
        "* What is the difference between null and saturated model?\n",
        "* Which model has greater log-likelihoood value?\n",
        "* Which model has the highest log-likelihood value?\n",
        "* What can you say about asymptotic distributions of $\\hat{\\beta}$ and $U(\\hat{\\beta})$ for saturated model?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download custom library from GitHub\n",
        "(using `wget` library)"
      ],
      "metadata": {
        "id": "iOc0DcmFG5SE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Please note that this cell works may not work in other env-s that Google Colab\n",
        "!pip install wget\n",
        "import wget\n",
        "url = \"https://github.com/francji1/01ZLMA/raw/main/code/helpers.py\"\n",
        "wget.download(url, '../content/helpers.py')  # path where Colab can find libraries"
      ],
      "metadata": {
        "id": "24VpxJrXG34x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um9ho8cQHobx"
      },
      "source": [
        "## Let's code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwHFMyUxOSqD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "from scipy import stats\n",
        "\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "import sklearn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.graphics.api import abline_plot\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from helpers import DiagnosticPlots, Anova\n",
        "\n",
        "sns.set_theme()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5J3NwU62uShi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: IWLS for Poisson Regression from the last week\n",
        "\n",
        "Generate synthetic data from a Poisson generalized linear model (GLM) using a canonical **log link**.\n",
        "\n",
        "- Set number of observations: $N = n*m=40$, $n = 1,\\ldots,20$ , $m=2$\n",
        "- Design matrix:\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & \\log(x_{2,1}) \\\\\n",
        "1 & \\log(x_{2,2}) \\\\\n",
        "\\vdots & \\vdots \\\\\n",
        "1 & \\log(x_{2,n})\n",
        "\\end{bmatrix},\\quad x_{2,i}=i\n",
        "$$\n",
        "- Choose regression coefficients:\n",
        "$$\n",
        "\\beta = \\begin{bmatrix} 0.5 \\\\ 1.2 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Generate response variable $Y$ from:\n",
        "$$\n",
        "\\lambda_i = e^{X_i\\beta}, \\quad Y_i \\sim \\text{Poisson}(\\lambda_i)\n",
        "$$\n",
        "\n",
        "\n",
        "- Manually implement the IWLS algorithm in Python (or R):\n",
        "\n",
        "  - Derive and Compute weights ($W$).\n",
        "  - Derive and Calculate adjusted response ($Z$).\n",
        "  - Derive and Write IWLS\n",
        "  - Update regression coefficients iteratively until convergence.\n",
        "  - Compare your IWLS estimates with a standard GLM package\n",
        "  -  discuss convergence, correctness, and interpretability of the results\n",
        "---\n"
      ],
      "metadata": {
        "id": "B25CfoBSuTH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "###  Theoretical Derivation\n",
        "\n",
        "Explicitly derive step-by-step:\n",
        "\n",
        "**Step 1: Model and Link Function**\n",
        "$$\n",
        "Y_i \\sim \\text{Poisson}(\\lambda_i), \\quad i = 1,2,\\dots,n \\times m\n",
        "$$\n",
        "\n",
        "\n",
        "- Canonical link function:\n",
        "$$\n",
        "g(\\lambda_i) = \\log(\\lambda_i) = X_i\\beta = \\beta_0 + \\beta_1 \\log(x_{2,i})\n",
        "$$\n",
        "\n",
        " - The mean parameter:\n",
        "\n",
        "$$\n",
        "\\lambda_i =  e^{X_i\\beta} = e^{\\beta_0}(x_{2,i})^{\\beta_1}\n",
        "$$\n",
        "\n",
        "\n",
        "**Step 2: Log-Likelihood**\n",
        "- The Poisson probability mass function for each observation $Y_i$:\n",
        "\n",
        "$$\n",
        "f(Y_i|\\lambda_i) = \\frac{\\lambda_i^{Y_i} e^{-\\lambda_i}}{Y_i!},\\quad i=1,2,\\dots,n\\times m\n",
        "$$\n",
        "- Log-Likelihood\n",
        "$$\n",
        "\\ell(\\beta) = \\sum_{i=1}^{n\\times m} \\left[ Y_i(X_i\\beta) - e^{X_i\\beta} - \\log(Y_i!) \\right]\n",
        "$$\n",
        "\n",
        "**Step 3: Score Function**\n",
        "\n",
        "$$\n",
        "U(\\beta) = \\frac{\\partial \\ell(\\beta)}{\\partial \\beta} = X^T(Y - \\mu), \\quad \\mu = e^{X\\beta}\n",
        "$$\n",
        "\n",
        "**Step 4: Fisher Information Matrix**\n",
        "\n",
        "$$\n",
        "I(\\beta) = X^T W X, \\quad W = \\text{diag}(\\mu)\n",
        "$$\n",
        "\n",
        "**Step 5: IWLS Update Equations**\n",
        "\n",
        "$$\n",
        "\\beta^{(t+1)} = \\beta^{(t)} + \\left[X^T W X\\right]^{-1} X^T W (Z - X\\beta^{(t)})\n",
        "$$\n",
        "with the adjusted response:\n",
        "$$\n",
        "Z = X\\beta^{(t)} + \\frac{Y - \\mu}{\\mu}\n",
        "$$\n",
        "\n",
        "Each iteration involves solving the weighted least squares equation:\n",
        "$$\n",
        "(X^T W X)\\beta^{(t+1)} = X^T W Z\n",
        "$$\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "GWCFJSBByinY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-kZTsg7FZoM"
      },
      "source": [
        "### Solution\n",
        "Solution based on Example 2 from the last Exercise 02"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fov4Z_w9OSqF"
      },
      "outputs": [],
      "source": [
        "n  = 20 # n observations\n",
        "m  = 2 # m parameters to estimate\n",
        "X1 = np.ones((n*m,))  # Intercept\n",
        "X2 = np.array([i for i in range(1, n+1)] * m) # Regressors\n",
        "X = np.vstack([X1, np.log(X2)]).T # design matrix\n",
        "beta = np.array([0.9, 1.3]) # Regression coefficients\n",
        "lamdas = np.exp(X @ beta) # Means\n",
        "Y = np.random.poisson(lamdas, n*m) # Response variable with Poisson distribution\n",
        "\n",
        "d = pd.DataFrame(data={'Y': Y, 'X1': X1, 'X2':X2})\n",
        "d.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = smf.glm(formula='Y~np.log(X2)', data=d, family=sm.families.Poisson()).fit()\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "id": "Le43pu3AIUQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5wSfH8H9JF_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# standard api requires specifying endog (response) and exog (explanatory) design matrices\n",
        "model = sm.GLM(endog=Y, exog=X, family=sm.families.Poisson()).fit()\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "id": "RrY2VHvbI6hW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot data\n",
        "beta_e = model.params; print(f'estimated params are:{beta_e}')\n",
        "y_hat = model.predict(); print(f'fitted values are:{y_hat}')\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X2, Y, color='red', marker='o')\n",
        "ax.plot(np.unique(y_hat), color='blue')\n",
        "ax.set_title('Poisson model')\n",
        "ax.set_xlabel('Time Index')\n",
        "ax.set_ylabel('Number of cases')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PVI-FZepI8b6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N8x-oi3HAKK"
      },
      "source": [
        "Repetition using custom function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSqY9UVsOSqG"
      },
      "outputs": [],
      "source": [
        "# function to calcualate weights W\n",
        "def calc_W_inv(X, beta):\n",
        "    return np.diag(np.exp(X @ beta))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcA9wuh1WMtL"
      },
      "outputs": [],
      "source": [
        "# function to calcualate weights Z\n",
        "def calc_Z(X,Y,beta):\n",
        "    return X@beta + (Y - np.exp(X@beta)) / np.exp(X@beta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igkUvl36XAGV"
      },
      "outputs": [],
      "source": [
        "# IWLS for example 2\n",
        "\n",
        "def IWLS(X,Y,beta_init,maxiter,epsilon):\n",
        "    res = {'FM': None, 'SV': None, 'betas': None}\n",
        "    # Fisher-scoring algorithm\n",
        "    i = 1     # first iteration\n",
        "\n",
        "    beta_i = beta_init\n",
        "\n",
        "    while i <= maxiter:\n",
        "        W = calc_W_inv(X,beta_i)\n",
        "        Z = calc_Z(X,Y,beta_i)\n",
        "        beta_pred = beta_i\n",
        "        beta_i = np.linalg.solve(X.T@W@X, X.T@W@Z)\n",
        "        diff = np.max(np.abs(beta_i - beta_pred))\n",
        "        if diff < epsilon:\n",
        "            break\n",
        "        W = calc_W_inv(X, beta_i)\n",
        "        Z = calc_Z(X, Y, beta_i)\n",
        "\n",
        "        res['SV'] = X.T@W@Z\n",
        "        res['FM'] = X.T@W@X\n",
        "        res['betas'] = np.linalg.solve(X.T@W@X, X.T@W@Z)\n",
        "        i += 1\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VDw0-mqZF7x"
      },
      "outputs": [],
      "source": [
        "# Estimation of betas\n",
        "result1 = IWLS(X,Y,np.ones(2),100,10^(-6))\n",
        "print(f'Estimation of parameters: {result1[\"betas\"]}')                # Estimation of parameters\n",
        "print(f'Estimated Fisher information matrix: {result1[\"FM\"]}')        # Estimated Fisher information matrix\n",
        "print(f'Estimated covariance matrix: {np.linalg.inv(result1[\"FM\"])}') # Estimated covariance matrix  = Inverse of estimated Fisher information matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEUlAqmFH_mB"
      },
      "source": [
        "Comparison of our custom solution with the built in glm function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6E-UbtQQXALw"
      },
      "outputs": [],
      "source": [
        "print(model.summary())\n",
        "FIM1 = model.cov_params()\n",
        "print(f'estimated covariance matrix {FIM1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMMvkPUTOSqI"
      },
      "outputs": [],
      "source": [
        "# to find out what params has `model` object\n",
        "for attr in dir(model):\n",
        "    if not attr.startswith('_'):\n",
        "        print(attr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c2y-MR7L5ElE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7vo_sJaOFiL"
      },
      "source": [
        "Asymptotics:\n",
        "\n",
        "* $ (\\hat{\\beta} - \\beta) \\sim N_{p}(0, I^{-1}(\\beta))$\n",
        "* Estimated Fisher information matrix  $\\hat{I}(\\hat{\\beta}) = (X^T \\hat{W} X)$  matrix.\n",
        "*  Estimated covariance matrix $\\hat{V} (\\hat{\\beta}) = (X^T \\hat{W} X)^{-1}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kB52Ef03Z7uO"
      },
      "outputs": [],
      "source": [
        "n = 10\n",
        "repet = 50\n",
        "n_observ = np.array([1,2,5,10,100, 500])\n",
        "betas_hat = np.zeros((6, repet, 2))\n",
        "\n",
        "for _, i in enumerate(n_observ):\n",
        "    for j in range(repet):\n",
        "        X1 = np.ones((n*i,))\n",
        "        X2 = np.array([i for i in range(1, n+1)]*i)\n",
        "        X  = np.vstack([X1, np.log(X2)]).T\n",
        "        beta = np.array([0.9, 1.3]) # Regression coefficients\n",
        "        lamdas = np.exp(X @ beta) # Means\n",
        "        Y = np.random.poisson(lamdas, n*i)\n",
        "        betas_hat[_, j] = sm.GLM(endog=Y, exog=X, family=sm.families.Poisson()).fit().params\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkafuLnXZ7xG"
      },
      "outputs": [],
      "source": [
        "\n",
        "for i in range(len(n_observ)):\n",
        "    print(f\"Number of observations: {n_observ[i]*n}\")\n",
        "    print(np.cov((betas_hat[i] - beta).T))\n",
        "    print(np.mean(betas_hat[i] - beta))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TjcOg79UPRM"
      },
      "source": [
        "## Hypothesis testing\n",
        "\n",
        "Use the model from the beginning again."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data generation\n",
        "#np.random.seed(0)\n",
        "#n = 30\n",
        "#X1 = np.ones(n)\n",
        "#X2 = np.log(np.arange(1, n + 1))\n",
        "#X = np.column_stack([X1, X2])\n",
        "#beta = np.array([0.5, 1.2])\n",
        "#mu = np.exp(X @ beta)\n",
        "#Y = np.random.poisson(mu)\n"
      ],
      "metadata": {
        "id": "_e22xynNABgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data generation\n",
        "n  = 20\n",
        "m  = 2\n",
        "\n",
        "X1 = np.ones((n*m,))\n",
        "X2 = np.array([i for i in range(1, n+1)]*m)\n",
        "X  = np.vstack([X1, np.log(X2)]).T\n",
        "mu = np.exp(X @ beta)\n",
        "beta = np.array([0.9, 1.3]) # Regression coefficients\n",
        "lamdas = np.exp(X @ beta) # Means\n",
        "Y = np.random.poisson(lamdas, n*m)\n"
      ],
      "metadata": {
        "id": "ZVJCdnZvABp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import chi2\n",
        "\n",
        "\n",
        "# Fit full and reduced models\n",
        "model_full = sm.GLM(Y, X, family=sm.families.Poisson()).fit()\n",
        "X_reduced = X[:, [0]]\n",
        "model_reduced = sm.GLM(Y, X_reduced, family=sm.families.Poisson()).fit()\n",
        "\n",
        "# Wald test (individual parameters)\n",
        "wald_stats = (model_full.params / model_full.bse)**2\n",
        "wald_pvalues = chi2.sf(wald_stats, 1)\n",
        "\n",
        "# Joint Wald test\n",
        "wald_stat_joint = model_full.params.T @ np.linalg.inv(model_full.cov_params()) @ model_full.params\n",
        "wald_pvalue_joint = chi2.sf(wald_stat_joint, len(model_full.params))\n",
        "\n",
        "# Score test (omit X2 manually)\n",
        "mu_reduced = model_reduced.mu\n",
        "score_vector = X[:, 1].T @ (Y - mu_reduced)\n",
        "I_reduced = (X[:, 1]**2 * mu_reduced).sum()\n",
        "score_stat = (score_vector**2) / I_reduced\n",
        "score_pvalue = chi2.sf(score_stat, 1)\n",
        "\n",
        "# Deviances and LRT (manual)\n",
        "deviance_full = model_full.deviance\n",
        "deviance_reduced = model_reduced.deviance\n",
        "lrt_stat = deviance_reduced - deviance_full\n",
        "lrt_df = int(model_reduced.df_model - model_full.df_model)\n",
        "lrt_pvalue = chi2.sf(lrt_stat, abs(lrt_df))\n",
        "\n",
        "# Built-in tests\n",
        "wald_test_sm = model_full.wald_test(np.eye(len(beta)), scalar=True)\n",
        "score_test_sm = model_reduced.score_test(X[:, [1]])\n",
        "\n",
        "# --- Results ---\n",
        "print(\"Manual Computations:\\n\")\n",
        "print(f\"Wald test Intercept: Statistic = {wald_stats[0]:.4f}, p-value = {wald_pvalues[0]:.4f}\")\n",
        "print(f\"Wald test X2: Statistic = {wald_stats[1]:.4f}, p-value = {wald_pvalues[1]:.4f}\")\n",
        "\n",
        "print(f\"\\nJoint Wald test: Statistic = {wald_stat_joint:.4f}, p-value = {wald_pvalue_joint:.4f}\")\n",
        "\n",
        "print(f\"\\nScore test (omit X2): Statistic = {score_stat:.4f}, p-value = {score_pvalue:.4f}\")\n",
        "\n",
        "print(f\"\\nDeviance (Full): {deviance_full:.4f}\")\n",
        "print(f\"Deviance (Reduced): {deviance_reduced:.4f}\")\n",
        "\n",
        "print(f\"\\nLikelihood Ratio Test (omit X2): Statistic = {lrt_stat:.4f}, p-value = {lrt_pvalue:.4f}, df = {abs(lrt_df)}\")\n",
        "\n",
        "print(\"\\nBuilt-in Tests from Statsmodels:\\n\")\n",
        "print(f\"Wald test (built-in): Statistic = {wald_test_sm.statistic:.4f}, p-value = {wald_test_sm.pvalue:.4f}\")\n",
        "\n",
        "# Correct handling of score_test_sm output\n",
        "score_stat_sm_scalar = score_test_sm[0][0][0] if score_test_sm[0].size > 1 else score_test_sm[0].item()\n",
        "score_pvalue_sm_scalar = score_test_sm[1].item()\n",
        "\n",
        "print(f\"\\nScore test (built-in): Statistic = {score_stat_sm_scalar:.4f}, p-value = {score_pvalue_sm_scalar:.4f}\")\n"
      ],
      "metadata": {
        "id": "9nypYTEwABsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rwWB0Grrmnt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from scipy.special import gammaln\n",
        "\n",
        "# Log-likelihood manually (matching model.llf)\n",
        "loglike_manual = np.sum(Y * np.log(model.mu) - model.mu - gammaln(Y + 1))\n",
        "print(\"Manual Log-Likelihood:\", loglike_manual)\n",
        "print(\"Built-in Log-Likelihood:\", model.llf)\n",
        "\n",
        "# Pearson chi2 manually\n",
        "pearson_chi2 = np.sum(((Y - model.mu)**2) / model.mu)\n",
        "print(\"Manual Pearson chi2:\", pearson_chi2)\n",
        "print(\"Built-in Pearson chi2:\", model.pearson_chi2)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2opFFdr0UfhS"
      },
      "outputs": [],
      "source": [
        "model = sm.GLM(endog=Y, exog=X, family=sm.families.Poisson()).fit()\n",
        "print(model.summary())\n",
        "\n",
        "# the unscaled (dispersion = 1) estimated covariance matrix of the estimated coefficients.\n",
        "FIM1 = model.cov_params()\n",
        "print(f'estimated covariance matrix {FIM1}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR3DTTTWV94T"
      },
      "source": [
        "**Calculation** of Z value\n",
        " $$Z_i = \\frac{\\hat{\\beta_i}}{(I^{-1}(\\hat{\\beta_i}))_{ii}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lv8guvybUK-E"
      },
      "outputs": [],
      "source": [
        "# Testing statistics from summary table\n",
        "print(model.summary())\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNjECz1FNw_c"
      },
      "outputs": [],
      "source": [
        "# By definition\n",
        "\n",
        "z_stat = model.params / np.sqrt(np.diag(model.cov_params()))\n",
        "print(z_stat)\n",
        "z_stat == model.tvalues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mHfejdWULDh"
      },
      "outputs": [],
      "source": [
        "# p-values of the test\n",
        "p_val = 2*scipy.stats.norm.sf(z_stat, loc=0, scale=1)\n",
        "print(f'pval: {p_val}')\n",
        "p_val == model.pvalues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GdfbeqdYOIv"
      },
      "outputs": [],
      "source": [
        "### 100(1-alpha) confidence interval\n",
        "alpha = 0.05\n",
        "u = scipy.stats.norm.ppf(1-alpha/2,0,1)\n",
        "CI_LB = model.params[1] - u * np.sqrt(np.diag(model.cov_params())[1])\n",
        "CI_UB = model.params[1] + u * np.sqrt(np.diag(model.cov_params())[1])\n",
        "\n",
        "print(f\"2.5% CI = {CI_LB},ESTIM = {model.params[1]}, 97.5% CI = {CI_UB}\")\n",
        "\n",
        "\n",
        "# built in function\n",
        "print(model.conf_int())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkhFJFXceHjn"
      },
      "source": [
        "Question:\n",
        "\n",
        "* Compare hypothesis testing in LM vs. GLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e4hAyql47eiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison of inference in Linear Regression (LR) and Generalized Linear Models (GLM)\n",
        "\n",
        "### Dimension notation:\n",
        "\n",
        "- $n$: number of observations (rows in dataset)\n",
        "- $p$: number of parameters (including intercept, columns in $X$)\n",
        "- $q$: number of parameters tested simultaneously in multiple-parameter tests (difference between full and reduced model parameters)\n",
        "\n",
        "---\n",
        "\n",
        "### Hypothesis tests summary table\n",
        "\n",
        "| Model Type       | Test type                      | Distribution         | Name(s) used       |\n",
        "|------------------|--------------------------------|----------------------|--------------------|\n",
        "| **Linear (OLS)** | Individual parameter           | $t_{n-p}$            | t-test             |\n",
        "| **Linear (OLS)** | Multiple parameters            | $F_{q,n-p}$          | F-test (ANOVA)     |\n",
        "| **GLM (MLE)**    | Individual parameter           | $N(0,1)$             | Wald test, z-test  |\n",
        "| **GLM (MLE)**    | Multiple parameters (Wald)     | $\\chi^2_q$           | Wald test          |\n",
        "| **GLM (MLE)**    | Likelihood Ratio Test (nested) | $\\chi^2_q$           | LRT, Chi-squared   |\n",
        "| **GLM (MLE)**    | Score (Rao) test               | $\\chi^2_q$           | Score test         |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "BiHSbvBd5FE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Comparison of inference in Linear Regression (LR) and Generalized Linear Models (GLM)\n",
        "\n",
        "| Aspect                      | Linear Regression (LR)                                 | Generalized Linear Models (GLM)                           |\n",
        "|-----------------------------|--------------------------------------------------------|------------------------------------------------------------|\n",
        "| **Model specification**     | $$Y = X\\beta + \\varepsilon,\\quad \\varepsilon \\sim N(0,\\sigma^2 I)$$ | $$g(\\mu) = X\\beta,\\quad Y\\sim\\text{Exponential family}$$  |\n",
        "| **Estimator type**          | OLS (also MLE under normality)                         | MLE                                                        |\n",
        "| **Variance estimation**     | Explicitly estimated: $$\\hat{\\sigma}^2 = \\frac{\\|Y - X\\hat{\\beta}\\|^2}{n-p}$$ | Implicitly determined by mean-variance relationship (no separate parameter) |\n",
        "| **Estimator distribution**  | Exact finite-sample: $$\\frac{\\hat{\\beta}_j - \\beta_j}{SE(\\hat{\\beta}_j)} \\sim t_{n-p}$$ | Asymptotic (large-sample): $$\\frac{\\hat{\\beta}_j - \\beta_j}{SE(\\hat{\\beta}_j)} \\xrightarrow{d} N(0,1)$$ |\n",
        "| **Test for single parameter** | $$t = \\frac{\\hat{\\beta}_j}{SE(\\hat{\\beta}_j)} \\sim t_{n-p}$$ | Wald test (z-test): $$Z = \\frac{\\hat{\\beta}_j}{SE(\\hat{\\beta}_j)}\\sim N(0,1)$$ |\n",
        "| **Test for multiple parameters** | F-test: $$F = \\frac{(RSS_0 - RSS_1)/q}{RSS_1/(n-p)}\\sim F_{q,n-p}$$ | Likelihood Ratio or Wald test: $$\\chi^2 = 2(l_{\\text{full}}-l_{\\text{reduced}})\\sim\\chi^2_q$$ |\n",
        "| **Small sample inference**  | Exact (t and F-distributions)                          | Approximate (not exact, relies on large-sample assumptions) |\n",
        "| **Large sample inference**  | t-distribution converges to normal (z-test)            | Normal (z-test, Wald test) approximation                   |\n",
        "\n",
        "                              |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "EhQJCtEi5zQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference and Hypothesis Tests in GLM (statsmodels)\n",
        "\n",
        "GLM inference typically relies on Maximum Likelihood Estimation (MLE) using iteratively reweighted least squares (IRLS).\n",
        "\n",
        "\n",
        "### Wald Test\n",
        "- **Formula:**\n",
        "  $$\n",
        "  W = (\\hat{\\beta} - \\beta_0)^T [I(\\hat{\\beta_0})]^{-1} (\\hat{\\beta} - \\beta_0)\n",
        "  $$\n",
        "- Evaluated using the covariance matrix (Hessian-based).\n",
        "- Assumes large-sample normality of parameter estimates.\n",
        "\n",
        "### Likelihood Ratio Test (LRT)\n",
        "- **Formula:**\n",
        "  $$\n",
        "  LR = 2(l_{full} - l_{reduced})\n",
        "  $$\n",
        "- Compares log-likelihoods of nested models.\n",
        "- More powerful if correctly specified; asymptotically follows $\\chi^2$ distribution.\n",
        "\n",
        "### Score (Rao) Test\n",
        "- **Formula:**\n",
        "  $$\n",
        "  R = s(\\hat{\\beta_0})^T [I(\\hat{\\beta_0})]^{-1}s(\\hat{\\beta_0})\n",
        "  $$\n",
        "  - Where $s$ is the gradient (score function), and $I$ is Fisher information.\n",
        "- Only requires the restricted model (does not fit the full model).\n",
        "- Computationally efficient for large models.\n",
        "\n",
        "## Deviance\n",
        "- **Definition:**\n",
        "  $$\n",
        "  D = 2(l_{sat} - l_{model})\n",
        "  $$\n",
        "  - Measures discrepancy between fitted model and the saturated (fully parameterized) model.\n",
        "- Used primarily to assess overall model fit, smaller deviance indicates better fit.\n",
        "\n"
      ],
      "metadata": {
        "id": "JSL2FRSU9cX8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwXaIg0peQee"
      },
      "source": [
        "# Deviance\n",
        "\n",
        "Deviance is a measure of goodness of fit of a GLM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjNUyZoZeStY"
      },
      "source": [
        "Log-likelihood of the saturated model is the highest possible one with given data, $\\tilde{\\mu}_i = y_i$ and $\\tilde{\\theta_i} = \\theta(y) = (b')^{-1}(y_i)$.\n",
        "$$l(\\tilde{\\mu},\\phi;y)=\\sum_{i=1}^{n}\\frac{y_{i}\\tilde{\\theta}_{i}-b(\\tilde{\\theta}_{i})}{a_{i}(\\phi)}+\\sum_{i=1}^{n}c(y_i,\\phi)$$\n",
        "\n",
        "Scale deviance statistics:\n",
        "$${S(y,\\hat{\\mu},\\phi)}=2\\left[l(\\tilde{\\mu},\\phi;y)-l(\\hat{\\mu},\\phi;y)\\right]\n",
        "=2\\sum_{i=1}^{n}\\frac{y_{i}(\\tilde{\\theta}_{i}-\\hat{\\theta}_{i})\n",
        "-\\left(b(\\tilde{\\theta}_{i})-b(\\hat{\\theta}_{i})\\right)}{a_{i}(\\phi)}.\n",
        "$$\n",
        "\n",
        "Deviance:\n",
        "Let $a_{i}(\\phi)=a_{i}\\phi$, then\n",
        "$$S(y,\\hat{\\mu},\\phi)=\\frac{D(y,\\hat{\\mu})}{\\phi},\n",
        "$$\n",
        "and\n",
        "$$\n",
        "D(y,\\hat{\\mu})=2\\sum_{i=1}^{n}\\frac{y_{i}(\\tilde{\\theta}_{i}-\\hat{\\theta}_{i})\n",
        "-\\left(b(\\tilde{\\theta}_{i})-b(\\hat{\\theta}_{i})\\right)}{a_{i}}\n",
        "$$\n",
        "\n",
        "### Comparison of two models\n",
        "\n",
        "Assume model $D_0$ with $p_0$ paramters and its sub-model $D_1$ with $p_1$ parameters, then\n",
        "$$ \\frac{1}{\\phi} (D_0 - D_1) \\sim \\chi_{(p_0 - p_1)} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLxMlbIKeTe5"
      },
      "source": [
        "Question:\n",
        "* Can we take deviance as a measure of the model quality?\n",
        "* Can we use deviance as a measure of the saturated model quality?\n",
        "* Complete the sentence: Compare two GLMs with deviance is like compare two LMs with ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PVvXp_SpDUx"
      },
      "outputs": [],
      "source": [
        "# Add random variable to the previous model\n",
        "Z = scipy.stats.uniform.rvs(loc=0, scale=1, size=n*m)\n",
        "model_0 = sm.GLM(endog=Y, exog=np.hstack([X, Z[:, None]]), family=sm.families.Poisson()).fit()\n",
        "print(model_0.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_O24NMRTPjoA"
      },
      "outputs": [],
      "source": [
        "# Proposed model\n",
        "m1 = sm.GLM(endog=Y, exog=X, family=sm.families.Poisson())\n",
        "model_1 = m1.fit()\n",
        "print(model_1.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbtdZzD6Pjx6"
      },
      "outputs": [],
      "source": [
        "# Null model\n",
        "\n",
        "model_n = sm.GLM(endog=Y, exog=X[:, 0], family=sm.families.Poisson()).fit()\n",
        "print(model_n.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "508GBQM_Pj5L"
      },
      "outputs": [],
      "source": [
        "# Saturated model CANNOT BY OBTAINED BY STATSMODELS BY DEFAULT 'CAUSE THEY PREVENT ZERO DIVISION\n",
        "\n",
        "I = np.diag(np.ones((m*n,)))\n",
        "\n",
        "model_s = sm.GLM(endog=Y, exog=I, family=sm.families.Poisson()).fit()\n",
        "print(model_s.summary())\n",
        "print(f'Residual deviance is: {model_s.deviance}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWfxVEHguydO"
      },
      "source": [
        "For Poisson model:\n",
        "$$D = 2 \\sum_{i=1}^n y_i log( \\frac{y_i}{\\hat{\\mu_i}})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Dm5NQ5Jr509"
      },
      "outputs": [],
      "source": [
        "mu_est_0 = model_0.predict()\n",
        "mu_est_1 = model_1.predict()\n",
        "\n",
        "Dev_0 = 2*np.sum(Y*np.log(Y/mu_est_0))\n",
        "print(Dev_0)\n",
        "Dev_1 = 2*np.sum(Y*np.log(Y/mu_est_1))\n",
        "print(Dev_1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNbCpDS0OSqN"
      },
      "outputs": [],
      "source": [
        "anova = Anova()\n",
        "anova(model_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9SuGMWFwhDM"
      },
      "source": [
        "## Anova testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8MegJJTsqvK"
      },
      "outputs": [],
      "source": [
        "display(anova(model_1))\n",
        "display(anova(model_1, test = \"Cp\"))\n",
        "display(anova(model_1, test = \"Chisq\"))\n",
        "\n",
        "display(anova(model_1, model_0, test = \"Rao\"))\n",
        "print(anova(model_1, model_0, test = \"LRT\"))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UchWeYjOxI7X"
      },
      "outputs": [],
      "source": [
        "# p-value of deviance tst\n",
        "# H0: model fit data\n",
        "p_dev = scipy.stats.chi2.sf(model_1.deviance, df=model_1.df_resid)\n",
        "\n",
        "print(p_dev)\n",
        "\n",
        "# critical value\n",
        "C_val = scipy.stats.chi2.isf(0.05, model_1.df_resid)\n",
        "print(C_val)\n",
        "\n",
        "print(model_1.summary())\n",
        "\n",
        "display(anova(model_1,model_s, test = \"LRT\"))   # saturated vs. final model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz4Ek6Y464bE"
      },
      "source": [
        "#### Rao score statistics (for Poisson GLM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrTIUoUKUKDg"
      },
      "outputs": [],
      "source": [
        "rao = np.sum((Y-model_1.predict())**2/model_1.predict())\n",
        "\n",
        "print(f'rao score statistic: {rao}')\n",
        "print(f'p-val of rao test: {scipy.stats.chi2.sf(rao, df=model_1.df_resid)}')\n",
        "\n",
        "######  By saturated model\n",
        "\n",
        "anova(model_1,model_s, test = \"Rao\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAmB4PDZJKEl"
      },
      "source": [
        "# Your turn:\n",
        "1. Generate data with followings parameters\n",
        " * $Y \\sim Poi(\\mu_i)$, where $E[Y_i] = \\mu_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} = x_i^T \\beta \\  \\Rightarrow \\ q(\\mu_i) = \\mu_i =  x_i^T \\beta  = \\eta_i$\n",
        "* $X_{i1} \\sim N(50,10)$\n",
        "* $X_{i2} \\sim U(10,60)$\n",
        "* $X_{i3} \\sim Ber(0.45)$\n",
        "* $n = 40$\n",
        "2. Compute $\\hat{\\mu_i}$  for saturated, null,\"full\",\"best\" models.\n",
        "3. Compute Deviance, Rao, Wald statistics for your model and compare final model with the saturated and \"full\" ones.\n",
        "4. Generate 100x data for  $n \\in \\{20,40,60,80,100 \\}$ and plot $(\\hat{\\beta_i} -\\beta_i)$ vs. $(n)$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3O60Qhn1rV7Z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1H9KmDWkOSqP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}